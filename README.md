# RAG-Langchain Project

## Description

This project implements a **Retriever-Augmented Generation (RAG)** application using **Langchain**, **Ollama**, and **Streamlit** to answer user queries based on document content. Users upload documents (e.g., PDFs, text files) and ask questions, with answers generated by combining **document retrieval** and **generative AI**.

## How the RAG Pipeline Works

### 1. **Document Upload & Preprocessing**
   - Upload a document (PDF or text file).
   - Extract text using **PyPDFLoader** or other loaders.

### 2. **Text Splitting**
   - Split text into smaller chunks using **RecursiveCharacterTextSplitter**.

### 3. **Vectorization**
   - Convert text chunks into embeddings using **Ollama Embeddings**.
   - Store embeddings in a **Chroma** vector store for fast retrieval.

### 4. **Retrieval**
   - Retrieve the most relevant document chunks from **Chroma** based on the user's query.

### 5. **Generation**
   - Pass the retrieved chunks to an **LLM (Ollama)** for generating an answer.

### 6. **Question Answering**
   - Use **RetrievalQA** to combine the retrieval and generation, delivering an accurate response.

---

## Pipeline
1. **User uploads a document** (PDF/Text).
2. **Document text is split** into smaller chunks.
3. **Text chunks are vectorized** using Ollama Embeddings.
4. **Chroma Vector Store** holds the embeddings for fast retrieval.
5. **Query submitted by user** is used to retrieve relevant document chunks.
6. **Answer generation** using LLM (Ollama).
7. **Final answer is displayed** in Streamlit.

## Installation

### Prerequisites

- Python 3.7+
- Streamlit
- Langchain
- Ollama API Key
- Chroma
